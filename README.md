# Re-examining the tradeoff between lexicon size and average morphosyntactic complexity in recursive numeral systems
This is the code for the CogSci 2025 oral presentation and paper above. Please contact davidmyang@berkeley.edu for any issues or questions.

### CSV File Grammars
There are a few differences to note between how the language grammars are structured in the csv files compared to the final paper. _Note that these differences do not change our results._
- In the csv files, the upper-bounds of ranges are exclusive, while they are inclusive in the final paper.
- In the csv files (and code), we use the term "monomorphemic" when referring to numerals like English 11. These are referred to as "suppletives" in the final paper.

### Code Guide
#### hurford_grammar.py
Generates constructions for numerals 1-99 for each language, using Hurford's grammar and language-specific constraints on top of that grammar.
- Input: Any csv file that specifies language-specific grammars (e.g. `data/natural_language_grammars.csv`). Note that this can be changed in `main()`.
- Output: File `data/language_specific_constructions.csv`.
- Also takes in an `is_last_gen` command-line boolean which specifies if this is the last generation of the artificial language generation process. 

#### artificial_language_generation.py
Randomly generates artificial languages and grammars.
- Takes in an `is_first_gen` command-line boolean which specifies if we need to generate artificial language grammars from scratch or mutate the previous generation.
- Input:
  - If first generation: No input file needed.
  - If later generation: A csv file with language-specific grammars to mutate (e.g. `data/artificial_language_grammars.csv`).
- Output:
  - If first generation: First-generation csv file (e.g. `data/first_gen_artificial_language_gramamrs.csv`).
  - If later generation: A csv file with language-specific grammars to mutate (e.g. `data/artificial_language_grammars.csv`). This is the same one that we read in as an input file for the next generation. We will elaborate on this in the `artificial_language_evolution.py` section.

#### complexity_analysis.py
Calculates the lexicon size and average morphosyntactic complexity (avg_ms_complexity) of languages. Note that the prior used in the avg_ms_complexity can be changed in `calculate_avg_ms_complexity()`.
- Input: A csv file for language grammars (e.g. `data/natural_language_grammars`) and the file containing language-specific constructions generated by `hurford_grammar.py` (`data/language_specific_constructions.csv`).
- Output: A csv file containing lexicon size and avg_ms_complexity values for all input languages (e.g. `data/language_analysis.csv`).
- There is some preliminary code for calculating grammar size, however, _this is not finalized and is not used in the paper._

#### artificial_language_evolution.py
Generates multiple generations of artificial languages using the scripts above and keeps the optimal languages from each generation.

#### prior_significance.py
Performs t-tests on the different priors and plots them (_Figure 4_ in paper).
- Input: A csv file containing lexicon size and avg_ms_complexity values (e.g. `data/pl_complexity.csv`).
- Output: Saved image (e.g. `image/priors_comparison.png`).

#### sum_optimization.py
Finds the optimal lambda value which minimizes the difference between an optimal _S(L*)_ and _S(L)_ for all natural languages _L_. Also generates a plot of the distribution (_Figure 2_ in paper).
- Input: A csv file containing lexicon size and avg_ms_complexity values (e.g. `data/language_analysis.csv`).
- Output: Optimal lambda value and saved image (`images/sum_plots.png`).

#### generate_plots.py
Creates plots of Pareto frontier (e.g. _Figure 1_ in paper).
- Input: A csv analysis file containing lexicon size and avg_ms_complexity values (e.g. `data/language_analysis.csv`).
- Output: Saved image (e.g. `image/test.png`).
